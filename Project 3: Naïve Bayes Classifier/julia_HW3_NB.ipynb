{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h2>Project 3: Na&iuml;ve Bayes and the Perceptron</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<blockquote>\n",
    "    <center>\n",
    "    <img src=\"nb.png\" width=\"200px\" />\n",
    "    </center>\n",
    "      <p><cite><center>\"All models are wrong, but some are useful.\"<br>\n",
    "       -- George E.P. Box\n",
    "      </center></cite></p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3>Introduction</h3>\n",
    "<!--Aðalbrandr-->\n",
    "\n",
    "<p>A&eth;albrandr is visiting America from Finland and has been having the hardest time distinguishing boys and girls because of the weird American names like Jack and Jane.  This has been causing lots of problems for A&eth;albrandr when he goes on dates. When he heard that Cornell has a Machine Learning class, he asked that we help him identify the gender of a person based on their name to the best of our ability.  In this project, you will implement Na&iuml;ve Bayes to predict if a name is male or female.</p>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<strong>How to submit:</strong> You can submit your code using the red <strong>Submit</strong> button above. This button will send any code below surrounded by <strong>#&lt;GRADED&gt;</strong><strong>#&lt;/GRADED&gt;</strong> tags below to the autograder, which will then run several tests over your code. By clicking on the <strong>Details</strong> dropdown next to the Submit button, you will be able to view your submission report once the autograder has completed running. This submission report contains a summary of the tests you have failed or passed, as well as a log of any errors generated by your code when we ran it.\n",
    "\n",
    "Note that this may take a while depending on how long your code takes to run! Once your code is submitted you may navigate away from the page as you desire -- the most recent submission report will always be available from the Details menu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p><strong>Academic Integrity:</strong> We will be checking your code against other submissions in the class for logical redundancy. If you copy someone else's code and submit it with minor changes, we will know. These cheat detectors are quite hard to fool, so please don't try. We trust you all to submit your own work only; <em>please</em> don't let us down. If you do, we will pursue the strongest consequences available to us.\n",
    "                </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p><strong>Getting Help:</strong> You are not alone!  If you find yourself stuck  on something, contact the course staff for help.  Office hours, section, and the <a href=\"https://piazza.com/class/iyag4nk2rsxsv\">Piazza</a> are there for your support; please use them.  If you can't make our office hours, let us know and we will schedule more.  We want these projects to be rewarding and instructional, not frustrating and demoralizing.  But, we don't know when or how to help unless you ask.  </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3> Of boys and girls </h3>\n",
    "\n",
    "<p> Take a look at the files <code>girls.train</code> and <code>boys.train</code>. For example with the unix command <pre>cat girls.train</pre> \n",
    "<pre>\n",
    "...\n",
    "Addisyn\n",
    "Danika\n",
    "Emilee\n",
    "Aurora\n",
    "Julianna\n",
    "Sophia\n",
    "Kaylyn\n",
    "Litzy\n",
    "Hadassah\n",
    "</pre>\n",
    "Believe it or not, these are all more or less common girl names. The problem with the current file is that the names are in plain text, which makes it hard for a machine learning algorithm to do anything useful with them. We therefore need to transform them into some vector format, where each name becomes a vector that represents a point in some high dimensional input space. </p>\n",
    "\n",
    "<p>That is exactly what the following Julia function <code>name2features</code> does: </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function name2features(filename;B=128,FIX=3,LoadFile=true)\n",
    "#  Output:\n",
    "#  X : n feature vectors of dimension B, (nxB)\n",
    "    \n",
    " function hashfeatures(baby)\n",
    "  v=zeros(B,1);\n",
    "  for m=1:FIX\n",
    "    featurestring=string(\"prefix\",baby[1:min(m,length(baby))]);\n",
    "    v[mod(hash(featurestring),B)+1]=1;\n",
    "    featurestring=string(\"suffix\",baby[end-min(m,length(baby))+1:end]);\n",
    "    v[mod(hash(featurestring),B)+1]=1;\n",
    "  end;\n",
    "  return(v)\n",
    " end;\n",
    "\n",
    "    # read in baby names\n",
    "    if LoadFile\n",
    "     f=open(filename);\n",
    "     babynames=[n for n=split(readstring(f),\"\\n\") if length(n)>0];\n",
    "     close(f);\n",
    "    else\n",
    "        babynames=split(filename,\"\\n\"); \n",
    "    end;\n",
    "    \n",
    "    # compute feature vectors\n",
    "    X=zeros(length(babynames),B);\n",
    "    for l=1:length(babynames)\n",
    "        X[l,:]=hashfeatures(babynames[l]);\n",
    "    end;\n",
    " return(X);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "It reads every name in the girls.train file and converts it into a 128-dimensional feature vector. </p> \n",
    "\n",
    "<p>Can you figure out what the features are? (Understanding how these features are constructed will help you later on in the competition.)<br></p>\n",
    "\n",
    "<p>We have provided you with a Julia function <code>genTrainFeatures</code>, which calls this script, transforms the names into features and loads them into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "function genTrainFeatures(dimension=128,fix=3);\n",
    "# This function calls the julia function name2features \n",
    "# to convert names into feature vectors and loads in the training data. \n",
    "#\n",
    "#\n",
    "# Output: \n",
    "#  x: n feature vectors of dimensionality d (nxd)\n",
    "#  y: n labels (+1 = girl, -1 = boy)\n",
    "#\n",
    "## Load in the data\n",
    "    Xgirls=name2features(\"girls.train\",B=dimension,FIX=fix);\n",
    "    Xboys=name2features(\"boys.train\",B=dimension,FIX=fix);\n",
    "    X=[Xgirls;Xboys];\n",
    "# Generate Labels\n",
    "    Y=[-ones(size(Xgirls,1),1);ones(size(Xboys,1),1)];\n",
    "\n",
    "# shuffle data into random order\n",
    "    ii=randperm(length(Y));\n",
    "    X=X[ii,:];\n",
    "    Y=Y[ii];\n",
    "    return(X,Y)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "You can call the following command to load in the features and the labels of all boys and girls names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X,Y=genTrainFeatures(128);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3> The Na&iuml;ve Bayes Classifier </h3>\n",
    "\n",
    "<p> The Na&iuml;ve Bayes classifier is a linear classifier based on Bayes Rule. The following questions will ask you to finish these functions in a pre-defined order. <br>\n",
    "<strong>As a general rule, you should avoid tight loops at all cost.</strong></p>\n",
    "<p>(a) Estimate the class probability P(Y) in \n",
    "<b><code>naivebayesPY</code></b>\n",
    ". This should return the probability that a sample in the training set is positive or negative, independent of its features.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function naivebayesPY(x,y)\n",
    "# Computation of P(Y)\n",
    "# Input:\n",
    "#  x : n input vectors of d dimensions (nxd)\n",
    "#  y : n labels (-1 or +1) (nx1)\n",
    "#\n",
    "# Output:\n",
    "#  pos: probability p(y=1)\n",
    "#  neg: probability p(y=-1)\n",
    "#\n",
    "# add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    y=[y;-1;1];\n",
    "    n=length(y);\n",
    "    ## fill in code here\n",
    "    \n",
    "    return (pos,neg)\n",
    "end;\n",
    "#</GRADED>\n",
    "pos,neg=naivebayesPY(X,Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(b) Estimate the conditional probabilities P(X|Y) in \n",
    "<b><code>naivebayesPXY</code></b>\n",
    ".  Use a <b>multinomial</b> distribution as model. This will return the probability vectors  for all features given a class label.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function naivebayesPXY(x,y)\n",
    "# Computation of P(X|Y)\n",
    "# Input:\n",
    "#  x : n input vectors of d dimensions (nxd)\n",
    "#  y : n labels (-1 or +1) (nx1)\n",
    "#\n",
    "# Output:\n",
    "#  posprob: probability vector of p(x|y=1) (1xd)\n",
    "#  negprob: probability vector of p(x|y=-1) (1xd)\n",
    "#\n",
    "# add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "\n",
    "    n,d=size(x);\n",
    "    x=[x;ones(2,d)];\n",
    "    y=[y;-1;1];\n",
    "    ## fill in code here\n",
    "\n",
    "    return (posprob,negprob)\n",
    "end;\n",
    "#</GRADED>\n",
    "\n",
    "posprob,negprob=naivebayesPXY(X,Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(c) Solve for the log ratio, $\\log\\left(\\frac{P(Y=1 | X)}{P(Y=-1|X)}\\right)$, using Bayes Rule.\n",
    " Implement this in \n",
    "<b><code>naivebayes</code></b>.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10 Array{Float64,2}:\n",
       " 0.861401  1.84994  2.83745  -0.118037  …  -1.96061  -0.477412  -2.73153"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#<GRADED>\n",
    "function naivebayes(x,y,xtest);\n",
    "# Computation of log P(Y|X=x1) using Bayes Rule\n",
    "# Input:\n",
    "#  x : n input vectors of d dimensions (nxd)\n",
    "#  y : n labels (-1 or +1)\n",
    "#  xtest: input vector of d dimensions (1xd)\n",
    "#\n",
    "# Output:\n",
    "#  logratio: log (P(Y = 1|X=x1)/P(Y=-1|X=x1))\n",
    "\n",
    "## fill in code here\n",
    "\n",
    "    return(logratio)\n",
    "end;\n",
    "#</GRADED>\n",
    "\n",
    "p=naivebayes(X,Y,transpose(X[1,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(d) Na&iuml;ve Bayes can also be written as a linear classifier.  Implement this in \n",
    "<b><code>naivebayesCL</code></b>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function naivebayesCL(x,y);\n",
    "# Implementation of a Naive Bayes classifier\n",
    "# Input:\n",
    "#  x : n input vectors of d dimensions (nxd)\n",
    "#  y : n labels (-1 or +1)\n",
    "\n",
    "# Output:\n",
    "#  w : weight vector\n",
    "#  b : bias (scalar)\n",
    "\n",
    "\n",
    "    n,d=size(x);\n",
    "    # fill in code here\n",
    "    \n",
    "    return (w[:],b)\n",
    "end;\n",
    "#</GRADED>\n",
    "\n",
    "w,b=naivebayesCL(X,Y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<p>(e) Implement \n",
    "<b><code>classifyLinear</code></b>\n",
    " that applies a linear weight vector and bias to a set of input vectors and outputs their predictions.  (You can use your answer from the previous project.)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 23.17%"
     ]
    }
   ],
   "source": [
    "#<GRADED>\n",
    "function  classifyLinear(x,w,b=0);\n",
    "# Make predictions with a linear classifier\n",
    "# Input:\n",
    "#  x : n input vectors of d dimensions (nxd)\n",
    "#  w : weight vector\n",
    "#  b : bias (optional)\n",
    "#\n",
    "# Output:\n",
    "#  preds: predictions\n",
    "#\n",
    "\n",
    "    ## fill in code here\n",
    "    \n",
    "    return(preds);\n",
    "end;\n",
    "#</GRADED>\n",
    "\n",
    "error=mean(classifyLinear(X,w,b).!=Y[:]);\n",
    "@printf(\"Training error: %2.2f%%\",error*100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "You can now test your code wih the following interactive name classification script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Training classifier ...\n",
      "Training error: 23.17%\n",
      "Please enter your name>STDIN> Mukund\n",
      "Mukund, I am sure you are a nice boy.\n",
      "\n",
      "Please enter your name>STDIN> Kilian\n",
      "Kilian, I am sure you are a nice boy.\n",
      "\n",
      "Please enter your name>STDIN> Jenny\n",
      "Jenny, I am sure you are a nice boy.\n",
      "\n",
      "Please enter your name>STDIN> Jennifer\n",
      "Jennifer, I am sure you are a nice boy.\n",
      "\n",
      "Please enter your name>STDIN> Anne\n",
      "Anne, I am sure you are a nice girl.\n",
      "\n",
      "Please enter your name>STDIN> Nika\n",
      "Nika, I am sure you are a nice girl.\n",
      "\n",
      "Please enter your name>STDIN> Lillian\n",
      "Lillian, I am sure you are a nice girl.\n",
      "\n",
      "Please enter your name>"
     ]
    }
   ],
   "source": [
    "DIMS=1200;\n",
    "println(\"Loading data ...\")\n",
    "X,Y=genTrainFeatures(DIMS);\n",
    "println(\"Training classifier ...\")\n",
    "w,b=naivebayesCL(X,Y);\n",
    "error=mean(classifyLinear(X,w,b).!=Y[:]);\n",
    "@printf(\"Training error: %2.2f%%\\n\",error*100);\n",
    "\n",
    "while true # stop only when user presses return\n",
    " print(\"Please enter your name>\")\n",
    " yourname=chomp(readline());\n",
    "    if length(yourname)==0;break;end;\n",
    " xtest=name2features(yourname,B=DIMS,LoadFile=false);\n",
    "    pred=classifyLinear(xtest,w,b)[1];\n",
    "    if pred>0\n",
    "        println(\"$yourname, I am sure you are a nice boy.\\n\")\n",
    " else\n",
    "        println(\"$yourname, I am sure you are a nice girl.\\n\")\n",
    " end;    \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "<h3> Feature Extraction (Competition)</h3>\n",
    "\n",
    "<p>(e) (<b>optional</b>) As always, this programming project also includes a competition.  We will rank all submissions by how well your Na&iuml;ve Bayes classifier performs on a secret test set. If you want to improve your classifier modify <code>name2features2</code> below.   The automatic reader will use your Julia function to extract features and train your classifier on the same names training set.  The given implementation is the same as the given <code>name2features</code> above.</p>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "function name2features2(filename;B=128,FIX=3,LoadFile=true)\n",
    "#  Output:\n",
    "#  X : n feature vectors of dimension B, (nxB)\n",
    "    \n",
    " function hashfeatures(baby)\n",
    "  v=zeros(B,1);\n",
    "  for m=1:FIX\n",
    "    featurestring=string(\"prefix\",baby[1:min(m,length(baby))]);\n",
    "    v[mod(hash(featurestring),B)+1]=1;\n",
    "    featurestring=string(\"suffix\",baby[end-min(m,length(baby))+1:end]);\n",
    "    v[mod(hash(featurestring),B)+1]=1;\n",
    "  end;\n",
    "  return(v)\n",
    " end;\n",
    "\n",
    "    # read in baby names\n",
    "    if LoadFile\n",
    "     f=open(filename);\n",
    "     babynames=[n for n=split(readstring(f),\"\\n\") if length(n)>0];\n",
    "     close(f);\n",
    "    else\n",
    "        babynames=split(filename,\"\\n\"); \n",
    "    end;\n",
    "    \n",
    "    # compute feature vectors\n",
    "    X=zeros(length(babynames),B);\n",
    "    for l=1:length(babynames)\n",
    "        X[l,:]=hashfeatures(babynames[l]);\n",
    "    end;\n",
    " return(X);\n",
    "end;\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Credits</h4>\n",
    "  Parts of this webpage were copied from or heavily inspired by John DeNero's and Dan Klein's (awesome) <a href=\"http://ai.berkeley.edu/project_overview.html\">Pacman class</a>. The name classification idea originates from <a href=\"http://nickm.com\">Nick Montfort</a>."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
